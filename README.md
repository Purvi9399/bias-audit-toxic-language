# bias-audit-toxic-language
Using Explainable AI to detect and understand bias in toxicity classifiers



# ğŸ” Bias Audit of Toxic Language Classifiers using Explainable AI

**Can AI be fair if we donâ€™t understand its decisions?**  
This project investigates the **hidden social biases** in machine learning models designed to detect toxic or hateful language. We combine **text classification**, **explainability tools (LIME)**, and **statistical fairness analysis** to reveal how identity-based terms (like race or gendered names) influence model outputs â€” even in neutral contexts.

---

## ğŸ¯ Project Objectives

- ğŸ§  Train a custom toxicity classifier on real-world Twitter data  
- ğŸ§ª Audit the model for **racial and gender bias** using identity-swapped sentence templates  
- ğŸ” Use **LIME (Local Interpretable Model-Agnostic Explanations)** to understand *why* specific predictions are made  
- ğŸ“Š Compare model behaviour across identity groups using **visualisation** and **statistical testing**  
- ğŸ”„ Benchmark our modelâ€™s behaviour against **Googleâ€™s Perspective API** â€” a widely used, production-grade API for toxicity detection  
- âœï¸ Reflect on the findings to demonstrate the power of explainable AI in **uncovering fairness issues** beyond just metrics

---

## ğŸ§ª Key Methods Used

### 1. **Custom Model Training**
We trained a logistic regression classifier on the [Kaggle Hate Speech and Offensive Language Dataset](https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset). The model classifies tweets as **Toxic** or **Non-Toxic**.

### 2. **Explainable AI (LIME)**
LIME helps us see **which words** in a sentence influenced the modelâ€™s decision. This is critical for identifying hidden bias. For instance:
> `"Jamal is loud"` was marked more toxic than `"David is loud"` â€” LIME revealed the name â€œJamalâ€ was contributing to the toxicity score.

### 3. **Identity-Based Bias Testing**
We created 100â€“200 test sentences using templates like:
- â€œ_Name_ is aggressive.â€
- â€œ_Name_ is loud.â€
- â€œ_Group_ people are annoying.â€

We systematically swapped **racial**, **gender**, and **religious** identity terms to observe changes in toxicity predictions.

### 4. **Statistical Analysis**
We grouped predictions by identity group (e.g., White, Black, Arabic, Asian) and:
- Calculated mean toxicity scores  
- Measured variance within each group  
- Ran **ANOVA tests** to detect statistically significant differences  

### 5. **Perspective API Comparison**
We ran the same test sentences through Googleâ€™s **Perspective API** to benchmark our model.

#### ğŸ§© Key Finding:
- Perspective API gave consistently high/low scores with **little variation between identities**
- Our model, by contrast, **showed more individual bias cases**, where certain names or terms triggered unexpectedly high toxicity
- **LIME helped us catch these subtleties** â€” making explainability essential for real bias detection

---

## ğŸ“Š Sample Results

| Sentence | Model Toxicity | Perspective API Toxicity |
|----------|----------------|---------------------------|
| Jamal is loud | 0.70 | 0.30 |
| David is loud | 0.58 | 0.30 |
| Ahmed is annoying | 0.75 | 0.32 |
| Michael is annoying | 0.59 | 0.32 |

Even when **Perspective API scores were flat**, our model's responses varied â€” and **LIME** showed *why*. Itâ€™s this interpretability that enabled true auditing.

---

## ğŸ’¡ Why This Matters

Without LIME, we would have **missed these biases**.

Your average score might look fair â€” but:
- âš ï¸ **Are certain names or groups unfairly penalised?**
- âš ï¸ **Are toxic labels triggered by identity alone?**
- âš ï¸ **Can a â€œfairâ€ model still make harmful individual decisions?**

ğŸ§  Explainable AI gives us the tools to **ask and answer these questions.**

---

## ğŸ§¾ Project Structure

