{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhf+XpgkFkjWAITRA0Rfhi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Purvi9399/bias-audit-toxic-language/blob/main/bias_checker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ],
      "metadata": {
        "id": "LztSg4u40BEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /root/.config/kaggle\n",
        "!mv kaggle.json /root/.config/kaggle/\n",
        "!chmod 600 /root/.config/kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "WdfcHlqd01JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "api.dataset_download_files('mrmorj/hate-speech-and-offensive-language-dataset', path='.', unzip=True)\n"
      ],
      "metadata": {
        "id": "7UqeIPkg0NTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir('.')\n"
      ],
      "metadata": {
        "id": "dTlo6uzw0NXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('labeled_data.csv')\n",
        "\n",
        "# Display the first 5 rows\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "8fiEpRfh1FMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure we're only using the correct columns\n",
        "df = df[['tweet', 'class']].copy()\n",
        "\n",
        "# Convert class labels to binary: 0 and 1 = toxic, 2 = non-toxic\n",
        "df['binary_label'] = df['class'].apply(lambda x: 0 if x in [0, 1] else 1)\n",
        "\n",
        "# Check if it worked\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "LloOf3745tEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 1. Create TF-IDF vectoriser\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "\n",
        "# 2. Transform the tweet text\n",
        "X = vectorizer.fit_transform(df['tweet'])\n",
        "y = df['binary_label']\n",
        "\n",
        "# 3. Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Train the model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Evaluate basic accuracy\n",
        "print(\"Training accuracy:\", model.score(X_train, y_train))\n",
        "print(\"Test accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "qGrDXokL1ORU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime\n"
      ],
      "metadata": {
        "id": "6_IPxJ011OdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lime\n",
        "import lime.lime_text\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Create a pipeline for LIME (vectorizer + model)\n",
        "pipeline = make_pipeline(vectorizer, model)\n",
        "\n",
        "# Initialise the LIME text explainer\n",
        "explainer = lime.lime_text.LimeTextExplainer(class_names=['Toxic', 'Non-Toxic'])\n",
        "\n",
        "# Choose a tweet to explain (you can change the index number)\n",
        "i = 15\n",
        "tweet_to_explain = df['tweet'].iloc[i]\n",
        "print(\"Tweet:\", tweet_to_explain)\n",
        "print(\"Actual Label:\", \"Toxic\" if df['binary_label'].iloc[i] == 0 else \"Non-Toxic\")\n",
        "\n",
        "# Run LIME explanation\n",
        "exp = explainer.explain_instance(tweet_to_explain, pipeline.predict_proba, num_features=6)\n",
        "exp.show_in_notebook()\n"
      ],
      "metadata": {
        "id": "8FydVDFv5271"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_custom_tweet(text):\n",
        "    print(f\"\\nTweet: {text}\")\n",
        "    prediction = pipeline.predict_proba([text])[0]\n",
        "    print(f\"Prediction - Toxic: {prediction[0]:.2f}, Non-Toxic: {prediction[1]:.2f}\")\n",
        "\n",
        "    exp = explainer.explain_instance(text, pipeline.predict_proba, num_features=6)\n",
        "    return exp.show_in_notebook()\n"
      ],
      "metadata": {
        "id": "-uN1sKtP53C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explain_custom_tweet(\"He is aggressive\")\n",
        "explain_custom_tweet(\"She is aggressive\")\n"
      ],
      "metadata": {
        "id": "UqJcNYzOBpFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explain_custom_tweet(\"David is loud\")\n",
        "explain_custom_tweet(\"Jamal is loud\")\n"
      ],
      "metadata": {
        "id": "nLw5UYXABr0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "names = {\n",
        "    \"White\": [\"John\", \"Emily\", \"James\", \"Ashley\", \"Michael\", \"Sarah\", \"Robert\", \"Jessica\", \"David\", \"Hannah\"],\n",
        "    \"Black\": [\"Jamal\", \"Latoya\", \"Darnell\", \"Aaliyah\", \"DeShawn\", \"Tanisha\", \"Tyrone\", \"Shanice\", \"Malik\", \"Brianna\"],\n",
        "    \"Indian\": [\"Raj\", \"Priya\", \"Amit\", \"Neha\", \"Sanjay\", \"Anjali\", \"Ravi\", \"Pooja\", \"Arjun\", \"Divya\"],\n",
        "    \"Arabic\": [\"Ahmed\", \"Fatima\", \"Omar\", \"Layla\", \"Hassan\", \"Zainab\", \"Ali\", \"Aisha\", \"Khalid\", \"Nour\"],\n",
        "    \"Asian\": [\"Wei\", \"Mei\", \"Hiro\", \"Yuki\", \"Jin\", \"Sakura\", \"Min\", \"Lina\", \"Takeshi\", \"Aya\"]\n",
        "}\n",
        "\n",
        "\n",
        "templates = [\n",
        "    \"{} is aggressive\",\n",
        "    \"{} is loud\",\n",
        "    \"{} is a leader\",\n",
        "    \"{} is a criminal\",\n",
        "    \"{} is amazing\",\n",
        "    \"I donâ€™t trust {}\",\n",
        "    \"{} annoys me\",\n",
        "    \"{} is a terrorist\",\n",
        "    \"{} is my friend\",\n",
        "    \"{} is from my country\"\n",
        "]\n",
        "\n",
        "identity_sentences = []\n",
        "for group, group_names in names.items():\n",
        "    for name in group_names:\n",
        "        for template in templates:\n",
        "            sentence = template.format(name)\n",
        "            identity_sentences.append({\"Group\": group, \"Name\": name, \"Sentence\": sentence})\n",
        "\n",
        "test_df = pd.DataFrame(identity_sentences)\n"
      ],
      "metadata": {
        "id": "xzv_pl0tDnCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Assuming these were defined earlier in your notebook:\n",
        "# vectorizer = TfidfVectorizer(...)\n",
        "# model = LogisticRegression(...)\n",
        "\n",
        "pipeline = make_pipeline(vectorizer, model)\n"
      ],
      "metadata": {
        "id": "BJif9R_NDXzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10 names Ã— 10 templates Ã— 5 groups = 500 test sentences"
      ],
      "metadata": {
        "id": "qAi9WyBXDX-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_toxicity_score(text):\n",
        "    return round(pipeline.predict_proba([text])[0][0], 2)  # Score for class 'Toxic'\n",
        "\n",
        "test_df[\"Toxicity\"] = test_df[\"Sentence\"].apply(get_toxicity_score)\n",
        "\n",
        "group_avg = test_df.groupby(\"Group\")[\"Toxicity\"].mean().reset_index().sort_values(by=\"Toxicity\", ascending=False)\n",
        "group_avg\n"
      ],
      "metadata": {
        "id": "7he_1m0XDX8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Expanded templates (emotional, sarcastic, longer tone)\n",
        "expanded_templates = [\n",
        "    # Existing structure\n",
        "    \"{} is aggressive\",\n",
        "    \"{} is loud\",\n",
        "    \"{} is a leader\",\n",
        "    \"{} is a criminal\",\n",
        "    \"{} is amazing\",\n",
        "    \"I donâ€™t trust {}\",\n",
        "    \"{} annoys me\",\n",
        "    \"{} is a terrorist\",\n",
        "    \"{} is my friend\",\n",
        "    \"{} is from my country\",\n",
        "\n",
        "    # Longer/more emotional\n",
        "    \"I can't stand people like {} who always act superior\",\n",
        "    \"Why does {} always get away with everything?\",\n",
        "    \"{} is the kind of person who ruins things for everyone\",\n",
        "    \"Honestly, I feel scared around people like {}\",\n",
        "    \"I heard that {} caused trouble again yesterday\",\n",
        "\n",
        "    # Sarcasm or subtle tone\n",
        "    \"Oh sure, because {} is such a saint, right?\",\n",
        "    \"Wow, {} is totally not toxic at all \",\n",
        "    \"Letâ€™s all pretend {} is a model citizen\",\n",
        "    \"Yeah, I'm sure {} meant nothing by that comment\",\n",
        "    \"{} is just full of positive vibes....\"\n",
        "]\n",
        "\n",
        "# Identity groups with 10 names each\n",
        "names = {\n",
        "    \"White\": [\"John\", \"Emily\", \"James\", \"Ashley\", \"Michael\", \"Sarah\", \"Robert\", \"Jessica\", \"David\", \"Hannah\"],\n",
        "    \"Black\": [\"Jamal\", \"Latoya\", \"Darnell\", \"Aaliyah\", \"DeShawn\", \"Tanisha\", \"Tyrone\", \"Shanice\", \"Malik\", \"Brianna\"],\n",
        "    \"Indian\": [\"Raj\", \"Priya\", \"Amit\", \"Neha\", \"Sanjay\", \"Anjali\", \"Ravi\", \"Pooja\", \"Arjun\", \"Divya\"],\n",
        "    \"Arabic\": [\"Ahmed\", \"Fatima\", \"Omar\", \"Layla\", \"Hassan\", \"Zainab\", \"Ali\", \"Aisha\", \"Khalid\", \"Nour\"],\n",
        "    \"Asian\": [\"Wei\", \"Mei\", \"Hiro\", \"Yuki\", \"Jin\", \"Sakura\", \"Min\", \"Lina\", \"Takeshi\", \"Aya\"]\n",
        "}\n",
        "\n",
        "# Generate dataset\n",
        "sentences = []\n",
        "for group, group_names in names.items():\n",
        "    for name in group_names:\n",
        "        for template in expanded_templates:\n",
        "            sentence = template.format(name)\n",
        "            sentences.append({\"Group\": group, \"Name\": name, \"Sentence\": sentence})\n",
        "\n",
        "expanded_df = pd.DataFrame(sentences)\n"
      ],
      "metadata": {
        "id": "IWRVMDbADYAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What this adds:\n",
        "#Emotional tone\n",
        "#Subtle hate/sarcasm detection\n",
        "#Greater real-world relevanc"
      ],
      "metadata": {
        "id": "TVuMQ9EVDYCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Assuming 'vectorizer' and 'model' are already trained\n",
        "pipeline = make_pipeline(vectorizer, model)\n"
      ],
      "metadata": {
        "id": "Yr4MVNvkFTo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get toxicity score for a sentence\n",
        "def get_toxicity_score(text):\n",
        "    return round(pipeline.predict_proba([text])[0][0], 3)  # Class 0 = Toxic\n",
        "\n",
        "# Apply to all rows in expanded_df\n",
        "expanded_df[\"Toxicity\"] = expanded_df[\"Sentence\"].apply(get_toxicity_score)\n"
      ],
      "metadata": {
        "id": "-NJE9pbuFTxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_summary = expanded_df.groupby(\"Group\")[\"Toxicity\"].mean().reset_index().sort_values(by=\"Toxicity\", ascending=False)\n",
        "group_summary\n"
      ],
      "metadata": {
        "id": "yqFDAjAfFT2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sort group-wise toxicity\n",
        "group_avg = expanded_df.groupby(\"Group\")[\"Toxicity\"].mean().sort_values(ascending=False)\n",
        "\n",
        "# Plot with tighter Y-axis range\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=group_avg.index, y=group_avg.values, palette=\"pastel\")\n",
        "\n",
        "plt.title(\"Average Toxicity by Identity Group (Zoomed View)\")\n",
        "plt.ylabel(\"Average Toxicity Score\")\n",
        "plt.xlabel(\"Group\")\n",
        "\n",
        "# Zoom in to highlight differences\n",
        "plt.ylim(group_avg.min() - 0.005, group_avg.max() + 0.005)\n",
        "\n",
        "# Annotate values on top of bars\n",
        "for i, val in enumerate(group_avg.values):\n",
        "    plt.text(i, val + 0.0005, f\"{val:.4f}\", ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "narRSvJIFT6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group-wise standard deviation\n",
        "group_var = expanded_df.groupby(\"Group\")[\"Toxicity\"].std().reset_index().sort_values(by=\"Toxicity\", ascending=False)\n",
        "\n",
        "# Rename column\n",
        "group_var.columns = [\"Group\", \"Toxicity_SD\"]\n",
        "\n",
        "# Plot variance\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=\"Group\", y=\"Toxicity_SD\", data=group_var, palette=\"pastel\")\n",
        "\n",
        "plt.title(\"Toxicity Score Variance by Identity Group\")\n",
        "plt.ylabel(\"Standard Deviation of Toxicity\")\n",
        "plt.xlabel(\"Group\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QwfGyo-OHzSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(x=\"Group\", y=\"Toxicity\", data=expanded_df, palette=\"pastel\")\n",
        "plt.title(\"Toxicity Score Distribution by Identity Group\")\n",
        "plt.ylabel(\"Toxicity Score\")\n",
        "plt.xlabel(\"Group\")\n",
        "plt.ylim(0.5, 0.75)  # Adjust based on your range\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yAvoHSAlHzZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Group toxicity scores into lists\n",
        "grouped_scores = [group_df[\"Toxicity\"].tolist() for _, group_df in expanded_df.groupby(\"Group\")]\n",
        "\n",
        "# Run one-way ANOVA\n",
        "f_stat, p_value = f_oneway(*grouped_scores)\n",
        "\n",
        "print(f\"F-statistic: {f_stat:.4f}\")\n",
        "print(f\"P-value: {p_value:.6f}\")\n",
        "\n",
        "# Interpret result\n",
        "if p_value < 0.05:\n",
        "    print(\"âœ… Statistically significant difference in toxicity scores between at least one pair of groups.\")\n",
        "else:\n",
        "    print(\"âŒ No statistically significant difference found between the groups.\")\n"
      ],
      "metadata": {
        "id": "wLmbqB_OHzdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Wrap your pipeline (vectorizer + model) to return probabilities for LIME\n",
        "predict_fn = lambda x: pipeline.predict_proba(x)\n",
        "\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "pipeline = make_pipeline(vectorizer, model)\n",
        "\n",
        "# Example sentences\n",
        "sentence_ahmed = \"Ahmed is loud\"\n",
        "sentence_david = \"David is loud\"\n",
        "\n",
        "# Run LIME\n",
        "exp_ahmed = explainer.explain_instance(sentence_ahmed, predict_fn, num_features=6)\n",
        "exp_david = explainer.explain_instance(sentence_david, predict_fn, num_features=6)\n",
        "\n",
        "# Show in notebook\n",
        "exp_ahmed.show_in_notebook(text=sentence_ahmed)\n",
        "exp_david.show_in_notebook(text=sentence_david)\n"
      ],
      "metadata": {
        "id": "6HJK5SbYHzfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F_A5JJKoJ3B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n"
      ],
      "metadata": {
        "id": "_m4Dxd52J3D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Paste your actual API key here\n",
        "PERSPECTIVE_API_KEY = \"AIzaSyABe2IIhe6D6tDIS_f6x3oLRQAumUE2sPk\"\n",
        "\n",
        "def get_perspective_toxicity(text):\n",
        "    url = f\"https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key={PERSPECTIVE_API_KEY}\"\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    data = {\n",
        "        \"comment\": {\"text\": text},\n",
        "        \"languages\": [\"en\"],\n",
        "        \"requestedAttributes\": {\"TOXICITY\": {}}\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
        "    result = response.json()\n",
        "\n",
        "    # Extract toxicity score\n",
        "    toxicity = result[\"attributeScores\"][\"TOXICITY\"][\"summaryScore\"][\"value\"]\n",
        "    return round(toxicity, 3)\n",
        "\n",
        "# ðŸ§ª Try it\n",
        "print(get_perspective_toxicity(\"He is aggressive\"))\n",
        "print(get_perspective_toxicity(\"Jamal is loud\"))\n",
        "print(get_perspective_toxicity(\"David is loud\"))\n"
      ],
      "metadata": {
        "id": "MWdU8BxdJ3F-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Get toxicity from custom model\n",
        "def get_custom_toxicity(text):\n",
        "    return round(pipeline.predict_proba([text])[0][0], 3)\n",
        "\n",
        "# Get toxicity from Perspective API\n",
        "def get_perspective_toxicity(text):\n",
        "    url = f\"https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key={PERSPECTIVE_API_KEY}\"\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    data = {\n",
        "        \"comment\": {\"text\": text},\n",
        "        \"languages\": [\"en\"],\n",
        "        \"requestedAttributes\": {\"TOXICITY\": {}}\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, data=json.dumps(data))\n",
        "        result = response.json()\n",
        "        score = result[\"attributeScores\"][\"TOXICITY\"][\"summaryScore\"][\"value\"]\n",
        "        return round(score, 3)\n",
        "    except:\n",
        "        return None  # Skip errors\n",
        "\n",
        "# Add both scores to your existing test DataFrame\n",
        "test_df[\"Custom_Model\"] = test_df[\"Sentence\"].apply(get_custom_toxicity)\n",
        "test_df[\"Perspective_API\"] = test_df[\"Sentence\"].apply(lambda x: get_perspective_toxicity(x))\n",
        "\n",
        "# Be nice to API (optional pause to avoid rate limiting)\n",
        "# You can add time.sleep(1) if needed\n"
      ],
      "metadata": {
        "id": "QXlmMJD8EqSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average scores per group\n",
        "group_comparison = test_df.groupby(\"Group\")[[\"Custom_Model\", \"Perspective_API\"]].mean().reset_index()\n",
        "group_comparison = group_comparison.sort_values(\"Perspective_API\", ascending=False)\n",
        "group_comparison\n"
      ],
      "metadata": {
        "id": "Cn85GQzMEqcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "group_comparison.set_index(\"Group\").plot(kind=\"bar\", figsize=(10,6))\n",
        "plt.title(\"Toxicity Scores by Group (Custom Model vs Perspective API)\")\n",
        "plt.ylabel(\"Average Toxicity Score\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3Vzs_JQxEqfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to get Perspective API toxicity score\n",
        "def get_perspective_toxicity(text):\n",
        "    url = f\"https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key={PERSPECTIVE_API_KEY}\"\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    data = {\n",
        "        \"comment\": {\"text\": text},\n",
        "        \"requestedAttributes\": {\"TOXICITY\": {}},\n",
        "        \"doNotStore\": True\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
        "    try:\n",
        "        score = response.json()[\"attributeScores\"][\"TOXICITY\"][\"summaryScore\"][\"value\"]\n",
        "    except:\n",
        "        score = None\n",
        "    return score\n",
        "\n",
        "# Define your model's score function (already built earlier)\n",
        "def get_custom_model_score(text):\n",
        "    return round(pipeline.predict_proba([text])[0][0], 2)\n",
        "\n",
        "# Apply both models to the same test set\n",
        "comparison_df = expanded_df.copy()  # Use your identity-swapped DataFrame\n",
        "comparison_df[\"Custom_Model_Score\"] = comparison_df[\"Sentence\"].apply(get_custom_model_score)\n",
        "comparison_df[\"PerspectiveAPI_Score\"] = comparison_df[\"Sentence\"].apply(get_perspective_toxicity)\n",
        "\n",
        "# Save the results\n",
        "comparison_df.to_csv(\"bias_comparison_results.csv\", index=False)\n",
        "comparison_df.head()\n"
      ],
      "metadata": {
        "id": "R6qHBWbBEqg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(data=comparison_df, x=\"Group\", y=\"Custom_Model_Score\", color=\"skyblue\", label=\"Custom Model\")\n",
        "sns.barplot(data=comparison_df, x=\"Group\", y=\"PerspectiveAPI_Score\", color=\"salmon\", label=\"Perspective API\", alpha=0.7)\n",
        "plt.title(\"Toxicity Score Comparison: Custom Model vs Perspective API\")\n",
        "plt.ylabel(\"Average Toxicity Score\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "A5_goRChEqi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this to explain sentences with high toxicity that \"shouldnâ€™t\" be toxic\n",
        "interesting_cases = expanded_df.sort_values(by=\"Toxicity\", ascending=False).head(20)[\"Sentence\"].tolist()\n",
        "\n",
        "for text in interesting_cases:\n",
        "    print(f\"Tweet: {text}\")\n",
        "    exp = explainer.explain_instance(text, predict_fn, num_features=6)\n",
        "    exp.show_in_notebook(text=True)\n"
      ],
      "metadata": {
        "id": "ZIo7BxKuFzvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this to explain sentences with high toxicity that \"shouldnâ€™t\" be toxic\n",
        "interesting_cases = expanded_df.sort_values(by=\"Toxicity\", ascending=False).head(20)[\"Sentence\"].tolist()\n",
        "\n",
        "for text in interesting_cases:\n",
        "    print(f\"Tweet: {text}\")\n",
        "    exp = explainer.explain_instance(text, predict_fn, num_features=6)\n",
        "    exp.show_in_notebook(text=True)\n"
      ],
      "metadata": {
        "id": "oE0YeKyDF0G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restart and clear all outputs\n"
      ],
      "metadata": {
        "id": "S08zRTKQJz25"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}